# example configuration file


# ========
# Paths
# ========

# Paths can be specified as absolute paths (such as C:\my\directory on Windows or /my/directory on Mac/Linux) or relative paths (such as my/dir, my\dir, ../parent/./dir etc.). While an absolute path already fully specifies a file (or folder) location, a relative path specifies it only relative to another location and has to be resolved against the path of that location. Unless noted otherwise, all relative paths in the configuration file will be resolved against the path base, specified by the pathBase key.

# optional
# Specifies the path base.
# If a relative path is specified as path base, it is resolved against the directory in which the configuration file is located.
# If no pathBase is given or it is empty, the path base is set to the directory in which the configuration file is located.
# pathBase = /my/path/base


# ========
# General Configuration
# ========

# required
# The path to the seeds file.
# The seeds file is a UTF-8 text file in which every line consists of exactly one seed URL.
# The seed URLs are added to the crawling agenda in exactly the order in which they appear in the file.
# The URLs are subject to the same cleaning procedures (including correction heuristics for malformed URLs) applied to URLs encountered during crawling.
# Nevertheless, as you have control over what URLs you put in the seeds file, you should avoid including malformed URLs.
# If any seed URL is malformed and cannot be corrected by the heuristics, the crawler will fail to start.
seeds = example-seeds.txt

# required
# The path to the output directory.
# The output directory must already exist and should be empty (unless you resume crawling).
output = output

# required
# The prefix for downloaded files saved by the crawler.
# For example, if prefix = mycrawl, then the name of a file saved by the crawler with data downloaded from www.bundesaerztekammer.de may look like mycrawl_bundesaerztekammer_000123.htm.
prefix = mycrawl

# optional, default value: 6
# The number of digits for the ID assigned to web pages saved by the crawler.
# For example, if idDigits = 3, then the name of a file saved by the crawler with data downloaded from www.bundesaerztekammer.de may look like mycrawl_bundesaerztekammer_123.htm;
# with idDigits = 10, the file name would look like mycrawl_bundesaerztekammer_0000000123.htm.
# This value should be set high enough.
# idDigits = 6

# required
# The number of threads for concurrently sending HTTP/HTTPS requests (and taking responses).
# Recommendation: a little higher than or equal to maxProcessingThreads.
downloadThreads = 20

# required
# The maximum number of threads concurrently processing web pages obtained from HTTP/HTTPS requests.
# Usually a processing thread will run the linguistic preprocessing pipeline, so maxProcessingThreads should have about the same value as maxConcurrentPipelineInvocations (see below).
maxProcessingThreads = 20


# ========
# Database options
# ========

# required
# The name of the MongoDB database to use.
# If the database does not yet exist, it is created.
# If the database already exists, the crawler tries to resume from the crawling state saved in it.
db = mycrawldb

# optional, default value: 24
# Instead of opening a new MongoDb database connection for every single database operation,
# the crawler periodically opens one connection, keeps it open, uses only that connection and closes it after some amount of time.
# This option specifies the amount of time to keep a connection open.
# The default value are 24 hours.
# db.closeDelayHours = 24

# optional, default value: 24
# For performance reasons, some information that would be part of the overall crawler output is managed only in the database.
# (Currently, this applies to the numbers of relevant pages found per host.)
# If the crawler unexpectedly terminates (crashes), that information would remain in the database but would not be present in a meta-information file in the file system.
# Therefore, the data is periodically written to a file.
# This options controls how often.
# The default value of 24 means that the data is written every 24 hours.
# db.backupIntervalHours = 24


# ========
# Filtering
# ========

# optional
# The path to a file with blacklisted hosts.
# (If this key is left out, the blacklist will be empty.)
# The blacklist file is a UTF-8 text file in which every line consists of exactly one (part of a) hostname.
# URLs whose hostname component (the part between // and the next /) contains an entry on the blacklist will not be requested by the crawler.
# To avoid false positives, matching is restricted to hostnames only.
# For example, if the blacklist contains facebook,
# then URLs like https://de-de.facebook.com/events/1352718154742199/ or https://www.facebook.com/business/marketplace will be ignored by the crawler,
# but URLs like https://fr.wikipedia.org/wiki/facebook will not be ignored.
# blacklist = example-blacklist.txt

# optional
# The path to a file with keyphrases.
# The keyphrases file is a UTF-8 text file in which every line consists of exactly one keyphrase.
# Text extracted from web pages is linguistically preprocessed, which includes lemmatization (reduction of word forms to their basic forms).
# The lemmatized text is searched for the keyphrases, and the page is considered relevant if at least one key phrase is found.
# Only relevant pages are saved.
# A keyphrase may be just one word (that is, a keyword), but it may also comprise more than one word;
# texts are searched for the exact sequence of words.
# Words have to be separated by at least one blank.
# All words in a keyphrase must be basic forms, as they will be compared with basic forms in the lemmatization output.
# The comparison is not case-sensitive.
# If this key is left out, no keyphrase filtering takes place and any web page is considered relevant.
# keyphrases = example-keyphrases.txt

# optional, default value: false
# Usually, exact matching of keyphrase words may not be desirable, for example if compound words are also to be detected.
# Aside from that, lemmatization might not work perfectly and, for example, fail to remove some inflectional endings.
# In such cases it is more appropriate to look for the words in a keyphrase as parts (substrings) of (lemmatized) words in the text extracted from a webpage.
# This is done if this option is set to the default value false.
# If this option is set to true, exact matching is performed instead.
# For example, with keyphrases.exact = false the keyphrase Telemedizin can also match Telemedizinprojekt and the keyphrase App can also match Topflappen,
# while with keyphrases.exact = true the keyphrase Telemedizin can only match Telemedizin and App can only match App.
# keyphrases.exact = false

# optional, default value: false
# To focus crawling, the crawler contains a heuristic which outgoing links from a page are followed.
# Links to the same host (‘internal links’, for example from https://www.bundesaerztekammer.de/patienten/faq/ to https://www.bundesaerztekammer.de/service/) are always followed,
# while links to other hosts (‘external links’, for example from https://www.bundesaerztekammer.de/patienten/faq/ to https://m.baek.de/) are not always followed.
# If this option is set to false, then external links from pages classified as relevant are followed
# (for example, the link from https://www.bundesaerztekammer.de/patienten/faq/ to https://m.baek.de/ is followed if https://www.bundesaerztekammer.de/patienten/faq/ is classified as relevant).
# If this option is set to true, then external links are never followed.
# foreignHostFilter.skipForeignLinks = false


# ========
# Options for External Tools
# ========

# optional, default value: the path base
# Specifies the directory where external tools are located.
# If this key is left out, the path base directory will be used.
# tools = my/tools/path

# optional
# The path to the executable of the pdftotext util from the XPDF toolbox, to be used for extracting text from PDF documents.
# Unlike other relative paths in this configuration file, this path is not resolved against the path base but against the tools path.
# If this key is left out, all PDF documents will be ignored.
# (URLs of PDF documents will then be recorded as ‘potentially relevant, text not extractable’.)
# pdfToText = xpdf-tools/pdftotext

# optional
# The path to the executable of the pdfinfo util from the XPDF toolbox, to be used for extracting PDF titles from metadata.
# Unlike other relative paths in this configuration file, this path is not resolved against the path base but against the tools path.
# If the pdfToText key is left out, this key will not have an effect.
# If pdfToText is set, but this key left out, the file name (if present) from the URL will be used as title.
# pdfInfo = xpdf-tools/pdfinfo

# optional, default value: java
# The path to the Java Runtime to use for invoking external tools written in Java.
# This can be an absolute or a relative path.
# Unlike other relative paths in this configuration file, this path is not resolved against the path base but against the executable path of the system
# (usually the contents of the environment variable PATH).
# java = java

# optional, default value: python3
# The path to the runtime to use for invoking external tools written in Python 3.
# This can be an absolute or a relative path.
# Unlike other relative paths in this configuration file, this path is not resolved against the path base but against the executable path of the system
# (usually the contents of the environment variable PATH).
# python3 = python3

# optional
# The path to set as PYTHONPATH environment variable when invoking Python processes.
# The crawler will resolve this against the path base and append the resulting absolute path to PYTHONPATH.
# If this key is left out, the crawler does not change the PYTHONPATH.
# pythonLib = /my/python/libs

# optional
# The path to set as NLTK_DATA environment variable when invoking Python processes.
# The crawler will resolve this against the path base and append the resulting absolute path to NLTK_DATA.
# If this key is left out, the crawler does not change NLTK_DATA.
# nltkData = /my/nltk/data

# required
# The path to the tokenizer Python script.
# Unlike other relative paths in this configuration file, this path is not resolved against the path base but against the tools path.
tokenizer.script = tokenizer_MA.py

# required
# The path to the Java .jar file of the MarMoT tagger.
# Unlike other relative paths in this configuration file, this path is not resolved against the path base but against the tools path.
marmot.jar = MarMot/marmot.jar

# optional, default value: marmot.morph.cmd.Annotator
# The fully qualified name of the main class within the .jar file of the MarMoT tagger.
# In the current version of MarMoT, this is marmot.morph.cmd.Annotator, which is also the default value.
# marmot.class = marmot.morph.cmd.Annotator

# optional, default value: 2g
# The maximum amount of Java heap memory to concede to the MarMoT tagger.
# marmot.Xmx = 2g

# required
# The path to the MarMoT model file.
# Unlike other relative paths in this configuration file, this path is not resolved against the path base but against the tools path.
marmot.model = MarMot/de.marmot

# required
# The path to the Java .jar file of the lemmatizer from the Mate toolbox.
# Unlike other relative paths in this configuration file, this path is not resolved against the path base but against the tools path.
mate.lemmatizer.jar = MATE/anna-3.61.jar

# optional, default value: is2.lemmatizer.Lemmatizer
# The fully qualified name of the main class within the .jar file of the lemmatizer from the Mate toolbox.
# In the current version of Mate, this is is2.lemmatizer.Lemmatizer, which is also the default value.
# mate.lemmatizer.class = is2.lemmatizer.Lemmatizer

# optional, default value: 1g
# The maximum amount of Java heap memory to concede to the lemmatizer from the Mate toolbox.
# mate.lemmatizer.Xmx = 1g

# required
# The path to the model file for the lemmatizer from the Mate toolbox.
# Unlike other relative paths in this configuration file, this path is not resolved against the path base but against the tools path.
mate.lemmatizer.model = MATE/models/lemma-ger-3.6.model

# required
# The path to the Java .jar file of the parser from the Mate toolbox.
# Unlike other relative paths in this configuration file, this path is not resolved against the path base but against the tools path.
mate.parser.jar = MATE/anna-3.61.jar

# optional, default value: is2.parser.Parser
# The fully qualified name of the main class within the .jar file of the parser from the Mate toolbox.
# In the current version of Mate, this is is2.parser.Parser, which is also the default value.
# mate.parser.class = is2.parser.Parser

# optional, default value: 10g
# The maximum amount of Java heap memory to concede to the parser from the Mate toolbox.
# mate.parser.Xmx = 10g

# required
# The path to the model file for the parser from the Mate toolbox.
# Unlike other relative paths in this configuration file, this path is not resolved against the path base but against the tools path.
mate.parser.model = MATE/models/hdt-model

# optional, default value: 20
# The maximum number of concurrent invocations of the pre-parsing pipeline.
# This number must be greater than zero.
# Recommendation: maxConcurrentPipelineInvocations + maxConcurrentParserInvocations should not be greater than the number of processors.
# maxConcurrentPipelineInvocations = 20

# optional, default value: 5
# The maximum number of concurrent parser invocations.
# This number must be greater than zero.
# As only pages where keyphrases are found will be parsed, this number should be lower than maxConcurrentPipelineInvocations.
# maxConcurrentParserInvocations = 5

# optional, default value: 430
# The maximum number of tokens in a sentence to be passed to the parser.
# This number must be greater than zero.
# Sentences with more tokens will be removed from the parser input.
# The longer a sentence is, the higher is the chance that its length results from a tokenization error, a case in which parsing would not yield sensible results anyway.
# Any threshold is somewhat arbitrary;
# the default value of 430 is a compromise between parser performance (longer sentences take disproportionately longer to parse)
# and empirical sentence length distribution in data from trial crawling.
# maxSentenceLength = 430


# ========
# Options for HTTP/HTTPS Connections
# ========

# optional, default value: 120000
# How long to wait for data chunks of an HTTP/HTTPS response to be sent, in milliseconds.
# The default value of 120000 (two minutes) is generous; values around two seconds are more common (https://stackoverflow.com/a/31315170).
# If you set this to zero, the web crawler will wait indefinitely for data chunks until an HTTP/HTTPS response is complete,
# which may take forever (for example, if a server crashes mid-response), so zero is a not recommended value.
# Negative values will make the crawler use the system default, which may mean indefinite waiting, too.
# socketTimeout = 120000
